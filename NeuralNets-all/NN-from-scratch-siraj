
# coding: utf-8

# In[2]:

import numpy as np


# In[3]:

def nonlin(x,deriv=False):
    if(deriv==True):
        return(np.exp(-z)/(1+np.exp(-z))**2)
    return 1/(1+np.exp(-x))
# whether we want the derivative or not , ofthe sigmoid
#we want derivative of the sigmoid only when we doback propogation to reduce the cost function


# In[4]:

# input data
x=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])
#output data
y=np.array([[0],[1],[1],[0]])


# In[5]:

#seed
np.random.seed(1)


# In[9]:

#synapses
syn0=2*np.random.random((3,4))-1 # 3 by 4 matrix with 1 as bias
syn1=2*np.random.random((4,1))-1 # here it is 4 by 1 matrix with same bias as before


# In[14]:

#training
for j in range(60000):
    #layers
    l0=x
    l1=nonlin(np.dot(l0,syn0))
    l2=nonlin(np.dot(l1,syn1))
    
    #backpropagation 
    l2_error=y-l2
    l2_delta=l2_error*nonlin(l2,deriv=True)
    l1_error=l2_delta.dot(syn1.T)    
    l1_delta=l1_error*nonlin(l1,deriv=True)
    
    # update synapses
    syn1 += l1.T.dot((l2_delta))
    syn0 += l0.T.dot((l1_delta))
    
print("output after training ")
print(l2)




